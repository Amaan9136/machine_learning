{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converted different language audio while maintaining the same tone and effect\n",
    "\n",
    "RUN BELOW AFTER RUNNING 'cmd.txt' file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLONE VOICE BY GIVING MY VOICE AND THEN SPEAK WITH MY SAME VOICE \"Hello My clone friend\" as replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_training/audio_1724777589.0895045.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioSegment\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplayback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m play\n\u001b[1;32m----> 4\u001b[0m sound \u001b[38;5;241m=\u001b[39m \u001b[43mAudioSegment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_training/audio_1724777589.0895045.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Get the original sample rate and channels\u001b[39;00m\n\u001b[0;32m      7\u001b[0m original_sample_rate \u001b[38;5;241m=\u001b[39m sound\u001b[38;5;241m.\u001b[39mframe_rate\n",
      "File \u001b[1;32mc:\\Users\\Amaan M k\\anaconda3\\envs\\gemini\\lib\\site-packages\\pydub\\audio_segment.py:651\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[1;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m file, close_file \u001b[38;5;241m=\u001b[39m \u001b[43m_fd_or_path_or_tempfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m:\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[1;32mc:\\Users\\Amaan M k\\anaconda3\\envs\\gemini\\lib\\site-packages\\pydub\\utils.py:60\u001b[0m, in \u001b[0;36m_fd_or_path_or_tempfile\u001b[1;34m(fd, mode, tempfile)\u001b[0m\n\u001b[0;32m     57\u001b[0m     close_fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fd, basestring):\n\u001b[1;32m---> 60\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     close_fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_training/audio_1724777589.0895045.wav'"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "sound = AudioSegment.from_file(\"data_training/audio_1724777589.0895045.wav\")\n",
    "\n",
    "# Get the original sample rate and channels\n",
    "original_sample_rate = sound.frame_rate\n",
    "original_channels = sound.channels\n",
    "\n",
    "print(original_sample_rate)\n",
    "print(original_channels)\n",
    "\n",
    "# Create a new AudioSegment with the same parameters\n",
    "noise_audio = AudioSegment(\n",
    "    data=sound.get_array_of_samples().tobytes(),\n",
    "    sample_width=sound.sample_width,\n",
    "    frame_rate=original_sample_rate,\n",
    "    channels=original_channels\n",
    ")\n",
    "\n",
    "play(noise_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from torchaudio.models.tacotron2 import Tacotron2\n",
    "from librosa import griffin_lim  \n",
    "\n",
    "# Load the audio file into a NumPy array\n",
    "sound, sr = librosa.load(\"data_training/audio_1724777589.0895045.wav\")\n",
    "\n",
    "# Load a pre-trained Tacotron2 model\n",
    "model = Tacotron2().eval()\n",
    "\n",
    "# Load the audio samples and extract features (MFCCs in this case)\n",
    "mfccs = librosa.extract_mfccs(sound.get_array_of_samples())\n",
    "\n",
    "# Convert MFCCs to a tensor for the model\n",
    "mfcc_tensor = torch.tensor(mfccs).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Use the model to generate mel-spectrogram\n",
    "mel_spectrogram = model(mfcc_tensor)[0]\n",
    "\n",
    "# Convert mel-spectrogram to waveform using a vocoder (e.g., Griffin-Lim)\n",
    "waveform = griffin_lim(mel_spectrogram)\n",
    "\n",
    "# Play the generated waveform using sounddevice\n",
    "import sounddevice as sd\n",
    "sd.play(waveform.numpy(), samplerate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.models.tacotron2 import Tacotron2\n",
    "from torchaudio.pipelines import tacotron2_pipeline\n",
    "from librosa import extract_mfccs\n",
    "from sounddevice import play\n",
    "\n",
    "# Load a pre-trained Tacotron2 model\n",
    "model = Tacotron2().eval()\n",
    "\n",
    "# Load the audio samples and extract features (MFCCs in this case)\n",
    "mfccs = librosa.extract_mfccs(sound.get_array_of_samples())\n",
    "\n",
    "# Convert MFCCs to a tensor for the model\n",
    "mfcc_tensor = torch.tensor(mfccs).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Use the model to generate mel-spectrogram\n",
    "mel_spectrogram = model(mfcc_tensor)[0]\n",
    "\n",
    "# Convert mel-spectrogram to waveform using a vocoder (e.g., Griffin-Lim)\n",
    "waveform = griffin_lim(mel_spectrogram)\n",
    "\n",
    "# Play the generated waveform\n",
    "play(waveform.numpy(), samplerate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pydub\n",
    "import numpy as np\n",
    "from gtts import gTTS\n",
    "import io\n",
    "from playsound import playsound\n",
    "\n",
    "def clone_voice(audio_file, new_song_file, to_clone):\n",
    "    \"\"\"\n",
    "    Clones a voice and generates a new song.\n",
    "\n",
    "    Args:\n",
    "        audio_file (str): Path to the audio file to clone.\n",
    "        new_song_file (str): Path to save the generated song.\n",
    "        to_clone (str): Text to be cloned using the cloned voice.\n",
    "    \"\"\"\n",
    "\n",
    "    y, sr = librosa.load(audio_file)\n",
    "\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr)\n",
    "\n",
    "    mfccs_contiguous = mfccs[0].copy()\n",
    "\n",
    "    new_song = np.sin(2 * np.pi * 440 * np.arange(len(y)) / sr)\n",
    "\n",
    "    modified_song = new_song * np.tile(np.exp(mfccs_contiguous), (len(new_song), 1)).T\n",
    "\n",
    "    # Normalize the modified song\n",
    "    modified_song = modified_song / np.max(np.abs(modified_song))\n",
    "\n",
    "    # Convert to pydub AudioSegment\n",
    "    modified_song_audio = pydub.AudioSegment.from_raw_data(modified_song.tobytes(), sample_rate=sr, channels=1)\n",
    "\n",
    "    cloned_text_audio = gTTS(text=to_clone, lang='en').audio_bytes\n",
    "\n",
    "    # Convert cloned_text_audio to a NumPy array\n",
    "    cloned_text_audio_np = np.array(pydub.AudioSegment.from_file(io.BytesIO(cloned_text_audio)).get_array_of_samples())\n",
    "\n",
    "    # Combine the modified song and cloned text audio\n",
    "    combined_audio = modified_song_audio + pydub.AudioSegment.from_array(cloned_text_audio_np, frame_rate=sr, channels=1)\n",
    "\n",
    "    combined_audio.export(new_song_file, format=\"wav\")\n",
    "\n",
    "    playsound(new_song_file)\n",
    "\n",
    "# Example usage\n",
    "clone_voice(\"data_training/audio_1724777589.0895045.wav\", \"cloned_song.wav\", to_clone=\"Hello How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound generation using numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "\n",
    "def generate_wave(wave_type=\"sine\", frequency=440, duration=1, sample_rate=44100):\n",
    "\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), False)\n",
    "\n",
    "    if wave_type == \"sine\":\n",
    "        wave = np.sin(2 * np.pi * frequency * t)\n",
    "    elif wave_type == \"noise\":\n",
    "        wave = np.random.rand(len(t)) * 32767 - 32768 \n",
    "    else:\n",
    "        raise ValueError(\"Invalid wave type. Choose 'sine' or 'noise'\")\n",
    "\n",
    "    return wave\n",
    "\n",
    "\n",
    "noise_samples = generate_wave(wave_type=\"noise\", sample_rate=44100)\n",
    "sine_samples = generate_wave(wave_type=\"sine\", sample_rate=44100)\n",
    "\n",
    "\n",
    "# Generate audio segments from samples\n",
    "noise_audio = AudioSegment(\n",
    "    data=noise_samples.tobytes(),\n",
    "    sample_width=2,\n",
    "    frame_rate=44100,\n",
    "    channels=1\n",
    ")\n",
    "\n",
    "sine_audio = AudioSegment(\n",
    "    data=sine_samples.tobytes(),\n",
    "    sample_width=2,\n",
    "    frame_rate=44100,\n",
    "    channels=1\n",
    ")\n",
    "\n",
    "\n",
    "# Plot the waveforms\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n",
    "ax1.plot(noise_samples, label=\"Noise\", linestyle=\"-\")\n",
    "ax1.set_title(\"Noise\")\n",
    "ax1.set_xlabel(\"Sample Index\")\n",
    "ax1.set_ylabel(\"Amplitude\")\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(sine_samples, label=\"Sine\", linestyle=\"--\", color=\"yellow\")\n",
    "ax2.set_title(\"Sine Wave\")\n",
    "ax2.set_xlabel(\"Sample Index\")\n",
    "ax2.set_ylabel(\"Amplitude\")\n",
    "ax2.grid(True)\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Play the generated noise\n",
    "play(noise_audio)\n",
    "# Play the generated sine\n",
    "play(sine_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "music audio always recorded 2 channel (left and right), but film audio can go anywhere from 5, 7 channel or more\n",
    "Two channels is the sweet spot for music because we have two ears.\n",
    "Multichannel is the sweet spot for film because it enhances the visual experience most.\n",
    "\n",
    "\n",
    "Values between -32768 and 32768:\n",
    "the highest number that can be represented in a signed 16-bit integer\n",
    "the range is (2^15-1) to (-2^15 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
